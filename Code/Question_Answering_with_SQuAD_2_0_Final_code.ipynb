{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Question_Answering_with_SQuAD_2_0_Final_code_with Example.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"703c1e81d50545208475e59177a9fb96":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4cf06d098e684f118d69f9c8e183a7ff","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c134352a443241308c175c1005a29ed5","IPY_MODEL_8146a92ebc0d4e4683ffa3bfcdc7103b"]}},"4cf06d098e684f118d69f9c8e183a7ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c134352a443241308c175c1005a29ed5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6406e954207d48a5a8542f9515dc9ebf","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_82e6b64168034bccbcbb20e2bda0357c"}},"8146a92ebc0d4e4683ffa3bfcdc7103b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8ef94398c7d44d8f8e9b666579e07dc9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 1.23MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8ab26b78483d4394a1bb98fa0a8a8ca8"}},"6406e954207d48a5a8542f9515dc9ebf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"82e6b64168034bccbcbb20e2bda0357c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8ef94398c7d44d8f8e9b666579e07dc9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8ab26b78483d4394a1bb98fa0a8a8ca8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ef28b932c5c9465c8efcd268894df642":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d1bc7c75a81e4906b3bddd033fd39754","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_09ba11302e3a4598a3f9d006a21b2f88","IPY_MODEL_abd9c800f67549e78f55b202b50da5d3"]}},"d1bc7c75a81e4906b3bddd033fd39754":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"09ba11302e3a4598a3f9d006a21b2f88":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9d4e7e297a154695b2af03617c1bb3b2","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_da706393b23f4bafa783fe32161d3f18"}},"abd9c800f67549e78f55b202b50da5d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_645dc530271c4e9485c419f48baa161b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 5.68MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1fd713cf3dc14b558044cc2f36440a39"}},"9d4e7e297a154695b2af03617c1bb3b2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"da706393b23f4bafa783fe32161d3f18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"645dc530271c4e9485c419f48baa161b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1fd713cf3dc14b558044cc2f36440a39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fac3f045fecf4161a6cf4b3dc1e57ba8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d508c288474143f3a92ee325f2bb81ca","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d71d9f1cd17c408c9a69251b4ccbbf51","IPY_MODEL_705143b6f2aa4b9798e55a5adfdb5189"]}},"d508c288474143f3a92ee325f2bb81ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d71d9f1cd17c408c9a69251b4ccbbf51":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_818a43f530964760b83cbe868166e021","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":442,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":442,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4ee4ae3fc75d4ea1b2e86fec6e002537"}},"705143b6f2aa4b9798e55a5adfdb5189":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5b5bdb585ace465490658a937c7c66e1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 442/442 [00:00&lt;00:00, 2.56kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_17ad826f138c4882a97a8ea12d74e877"}},"818a43f530964760b83cbe868166e021":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4ee4ae3fc75d4ea1b2e86fec6e002537":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5b5bdb585ace465490658a937c7c66e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"17ad826f138c4882a97a8ea12d74e877":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c27947961f2d4780acf46d7df051fefa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_40d143bd64e643edbb7e52d77ca040ad","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1340786af1c24a1daa53301a7a51c900","IPY_MODEL_626a29166a034294aead8337a3faadb2"]}},"40d143bd64e643edbb7e52d77ca040ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1340786af1c24a1daa53301a7a51c900":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5fcac64eac2a41ada39110c431fede11","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":363423424,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":363423424,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a1a5cb6748bf4cea87fec0630f6ccfd4"}},"626a29166a034294aead8337a3faadb2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_49c1cdd7a6794d37a99210b44243fe7f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 363M/363M [00:05&lt;00:00, 70.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_86ad474470ec456db833be3c0fdd685d"}},"5fcac64eac2a41ada39110c431fede11":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a1a5cb6748bf4cea87fec0630f6ccfd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"49c1cdd7a6794d37a99210b44243fe7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"86ad474470ec456db833be3c0fdd685d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"-Tz7wt7G_kLd"},"source":["**Question answering** comes in many forms. In this example, we’ll look at the particular type of extractive QA that involves answering a question about a passage by highlighting the segment of the passage that answers the question. This involves fine-tuning a model which predicts a start position and an end position in the passage. We will use the Stanford Question Answering Dataset (SQuAD) 2.0.\n","\n","We will start by downloading the data:"]},{"cell_type":"markdown","metadata":{"id":"ZKgPO4AUDZYi"},"source":["## **Note :**"]},{"cell_type":"markdown","metadata":{"id":"xAv2_F8VDL9t"},"source":["Please write your code in the cells with the \"**Your code here**\" placeholder."]},{"cell_type":"markdown","metadata":{"id":"ockcf0NvAHy5"},"source":["## **Download SQuAD 2.0 Data**"]},{"cell_type":"markdown","metadata":{"id":"6pmSMjQN_5zd"},"source":["Note : This dataset can be explored in the Hugging Face model hub (SQuAD V2), and can be alternatively downloaded with the 🤗 NLP library with load_dataset(\"squad_v2\")."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5j1mQEFlHKNE","executionInfo":{"status":"ok","timestamp":1609756202464,"user_tz":-330,"elapsed":1228,"user":{"displayName":"TINA GHOSH","photoUrl":"","userId":"17633624193521379699"}},"outputId":"6457b6e4-ed2a-444c-a155-c1a1c208a003"},"source":["!mkdir squad\n","!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad/train-v2.0.json\n","!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squad/dev-v2.0.json"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2021-01-04 10:30:01--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n","Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.111.153, 185.199.109.153, ...\n","Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 42123633 (40M) [application/json]\n","Saving to: ‘squad/train-v2.0.json’\n","\n","squad/train-v2.0.js 100%[===================>]  40.17M   242MB/s    in 0.2s    \n","\n","2021-01-04 10:30:02 (242 MB/s) - ‘squad/train-v2.0.json’ saved [42123633/42123633]\n","\n","--2021-01-04 10:30:02--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n","Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.111.153, 185.199.109.153, ...\n","Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4370528 (4.2M) [application/json]\n","Saving to: ‘squad/dev-v2.0.json’\n","\n","squad/dev-v2.0.json 100%[===================>]   4.17M  --.-KB/s    in 0.05s   \n","\n","2021-01-04 10:30:02 (83.0 MB/s) - ‘squad/dev-v2.0.json’ saved [4370528/4370528]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8zaAg1QkISyQ","executionInfo":{"status":"ok","timestamp":1609756210865,"user_tz":-330,"elapsed":7109,"user":{"displayName":"TINA GHOSH","photoUrl":"","userId":"17633624193521379699"}},"outputId":"6cc5943a-ffde-4cf6-f794-af0c8f5ae6a1"},"source":["!pip install transformers==4.0.1"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting transformers==4.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n","\u001b[K     |████████████████████████████████| 1.4MB 14.0MB/s \n","\u001b[?25hCollecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 51.2MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (1.19.4)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 49.4MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (0.8)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.1) (20.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.1) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.1) (2020.12.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.0.1) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.0.1) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.0.1) (1.0.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.0.1) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=2911c9d6b717c10861733bc61396108bf7eb85ef64a5a53b97b9e9854e0444cb\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4Jv7ipy5AYUM"},"source":["Each split is in a structured json file with a number of questions and answers for each passage (or context). We’ll take this apart into parallel lists of contexts, questions, and answers (note that the contexts here are repeated since there are multiple questions per context):"]},{"cell_type":"code","metadata":{"id":"IKMDLA8woiA8","executionInfo":{"status":"ok","timestamp":1609756225059,"user_tz":-330,"elapsed":1793,"user":{"displayName":"TINA GHOSH","photoUrl":"","userId":"17633624193521379699"}}},"source":["import json\n","from pathlib import Path\n","\n","def read_squad(path):\n","    path = Path(path)\n","    with open(path, 'rb') as f:\n","        squad_dict = json.load(f)\n","\n","    contexts = []\n","    questions = []\n","    answers = []\n","    for group in squad_dict['data']:\n","        for passage in group['paragraphs']:\n","            context = passage['context']\n","            for qa in passage['qas']:\n","                question = qa['question']\n","                for answer in qa['answers']:\n","                    contexts.append(context)\n","                    questions.append(question)\n","                    answers.append(answer)\n","\n","    return contexts, questions, answers\n","\n","train_contexts, train_questions, train_answers = read_squad('squad/train-v2.0.json')\n","val_contexts, val_questions, val_answers = read_squad('squad/dev-v2.0.json')\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2bPtDyaFAtQm"},"source":["The contexts and questions are just strings. The answers are dicts containing the subsequence of the passage with the correct answer as well as an integer indicating the character at which the answer begins. In order to train a model on this data we need (1) the tokenized context/question pairs, and (2) integers indicating at which token positions the answer begins and ends.\n","\n","First, let’s get the character position at which the answer ends in the passage (we are given the starting position). Sometimes SQuAD answers are off by one or two characters, so we will also adjust for that."]},{"cell_type":"code","metadata":{"id":"nraxKhioA1rG","executionInfo":{"status":"ok","timestamp":1609756228366,"user_tz":-330,"elapsed":791,"user":{"displayName":"TINA GHOSH","photoUrl":"","userId":"17633624193521379699"}}},"source":["def add_end_idx(answers, contexts):      \n","      for answer, context in zip(answers, contexts):\n","        gold_text = answer['text']\n","        start_idx = answer['answer_start']\n","        end_idx = start_idx + len(gold_text)\n","\n","        # sometimes squad answers are off by a character or two – fix this\n","        if context[start_idx:end_idx] == gold_text:\n","            answer['answer_end'] = end_idx\n","        elif context[start_idx-1:end_idx-1] == gold_text:\n","            answer['answer_start'] = start_idx - 1\n","            answer['answer_end'] = end_idx - 1    \n","        elif context[start_idx-2:end_idx-2] == gold_text:\n","            answer['answer_start'] = start_idx - 2\n","            answer['answer_end'] = end_idx - 2    \n","\n","add_end_idx(train_answers, train_contexts)\n","add_end_idx(val_answers, val_contexts)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CmLsa8hQA2a7"},"source":["Now train_answers and val_answers include the character end positions and the corrected start positions. Next, let’s tokenize our context/question pairs. 🤗 Tokenizers can accept parallel lists of sequences and encode them together as sequence pairs."]},{"cell_type":"code","metadata":{"id":"KdhZS6o0qMty","colab":{"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["703c1e81d50545208475e59177a9fb96","4cf06d098e684f118d69f9c8e183a7ff","c134352a443241308c175c1005a29ed5","8146a92ebc0d4e4683ffa3bfcdc7103b","6406e954207d48a5a8542f9515dc9ebf","82e6b64168034bccbcbb20e2bda0357c","8ef94398c7d44d8f8e9b666579e07dc9","8ab26b78483d4394a1bb98fa0a8a8ca8","ef28b932c5c9465c8efcd268894df642","d1bc7c75a81e4906b3bddd033fd39754","09ba11302e3a4598a3f9d006a21b2f88","abd9c800f67549e78f55b202b50da5d3","9d4e7e297a154695b2af03617c1bb3b2","da706393b23f4bafa783fe32161d3f18","645dc530271c4e9485c419f48baa161b","1fd713cf3dc14b558044cc2f36440a39"]},"executionInfo":{"status":"ok","timestamp":1609756285429,"user_tz":-330,"elapsed":54633,"user":{"displayName":"TINA GHOSH","photoUrl":"","userId":"17633624193521379699"}},"outputId":"0e5c6f32-3dda-461f-809f-6a7321076297"},"source":["from transformers import DistilBertTokenizerFast\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","\n","# Your code here\n","train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n","\n","# Your code here\n","val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"703c1e81d50545208475e59177a9fb96","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ef28b932c5c9465c8efcd268894df642","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DJdAikOIA_Zo"},"source":["Next we need to convert our character start/end positions to token start/end positions. When using 🤗 Fast Tokenizers, we can use the <b>built in char_to_token()</b> method."]},{"cell_type":"code","metadata":{"id":"Kau7PrtIBAde","executionInfo":{"status":"ok","timestamp":1609756326758,"user_tz":-330,"elapsed":915,"user":{"displayName":"TINA GHOSH","photoUrl":"","userId":"17633624193521379699"}}},"source":["def add_token_positions(encodings, answers):\n","    start_positions = []\n","    end_positions = []\n","    \n","    for i in range (len(answers)):\n","        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n","        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n","       \n","        if start_positions[-1] is None:\n","            start_positions[-1] = tokenizer.model_max_length-1\n","        if end_positions[-1] is None:\n","            end_positions[-1] = tokenizer.model_max_length-1\n","\n","    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n","\n","add_token_positions(train_encodings, train_answers)\n","add_token_positions(val_encodings, val_answers)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QLIp2RhABITo"},"source":["Our data is ready. Let’s just put it in a PyTorch/TensorFlow dataset so that we can easily use it for training. In PyTorch, we define a custom Dataset class. In TensorFlow, we pass a tuple of (inputs_dict, labels_dict) to the from_tensor_slices method."]},{"cell_type":"code","metadata":{"id":"cchIqVWpBI7X","executionInfo":{"status":"ok","timestamp":1609756633540,"user_tz":-330,"elapsed":303849,"user":{"displayName":"TINA GHOSH","photoUrl":"","userId":"17633624193521379699"}}},"source":["import tensorflow as tf\n","\n","# Your code here\n","train_dataset = tf.data.Dataset.from_tensor_slices((\n","    {key: train_encodings[key] for key in ['input_ids', 'attention_mask']},\n","    {key: train_encodings[key] for key in ['start_positions', 'end_positions']}\n","))\n","\n","# Your code here\n","val_dataset = tf.data.Dataset.from_tensor_slices((\n","    {key: val_encodings[key] for key in ['input_ids', 'attention_mask']},\n","    {key: val_encodings[key] for key in ['start_positions', 'end_positions']}\n","))"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OshKaYppBOtD"},"source":["Now we can use a DistilBert model with a QA head for training:"]},{"cell_type":"code","metadata":{"id":"-sGzpUDvBPVU","colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["fac3f045fecf4161a6cf4b3dc1e57ba8","d508c288474143f3a92ee325f2bb81ca","d71d9f1cd17c408c9a69251b4ccbbf51","705143b6f2aa4b9798e55a5adfdb5189","818a43f530964760b83cbe868166e021","4ee4ae3fc75d4ea1b2e86fec6e002537","5b5bdb585ace465490658a937c7c66e1","17ad826f138c4882a97a8ea12d74e877","c27947961f2d4780acf46d7df051fefa","40d143bd64e643edbb7e52d77ca040ad","1340786af1c24a1daa53301a7a51c900","626a29166a034294aead8337a3faadb2","5fcac64eac2a41ada39110c431fede11","a1a5cb6748bf4cea87fec0630f6ccfd4","49c1cdd7a6794d37a99210b44243fe7f","86ad474470ec456db833be3c0fdd685d"]},"executionInfo":{"status":"ok","timestamp":1609756652753,"user_tz":-330,"elapsed":8450,"user":{"displayName":"TINA GHOSH","photoUrl":"","userId":"17633624193521379699"}},"outputId":"3f69465a-5b86-47be-c1ac-d0809b88ceb7"},"source":["from transformers import TFDistilBertForQuestionAnswering\n","\n","# Your code here\n","model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"],"execution_count":8,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fac3f045fecf4161a6cf4b3dc1e57ba8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c27947961f2d4780acf46d7df051fefa","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForQuestionAnswering: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n","- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'qa_outputs']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"2UGXo8SwBTeX"},"source":["The data and model are both ready to go. You can train the model with Trainer/TFTrainer exactly as in the sequence classification example above. If using native PyTorch, replace labels with start_positions and end_positions in the training example. If using Keras’s fit, we need to make a minor modification to handle this example since it involves multiple model outputs."]},{"cell_type":"code","metadata":{"id":"lSwoxCD_BU_D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609773590920,"user_tz":-330,"elapsed":16904211,"user":{"displayName":"TINA GHOSH","photoUrl":"","userId":"17633624193521379699"}},"outputId":"de0279c9-2943-4039-a5bd-518c41d0b153"},"source":["# Keras will expect a tuple when dealing with labels\n","\n","# Write your code here to replace labels with start_positions and end_positions in the training example\n","train_dataset = train_dataset.map(lambda x, y: (x, (y['start_positions'], y['end_positions'])))\n","\n","# Keras will assign a separate loss for each output and add them together. So we'll just use the standard CE loss\n","# instead of using the built-in model.compute_loss, which expects a dict of outputs and averages the two terms.\n","# Note that this means the loss will be 2x of when using TFTrainer since we're adding instead of averaging them.\n","\n","# Your code here\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","model.distilbert.return_dict = False\n","\n","# Your code here\n","optimizer = tf.keras.optimizers.Adam(learning_rate=4e-6)\n","\n","model.compile(optimizer=optimizer, loss=loss) \n","model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","5427/5427 [==============================] - 5648s 1s/step - loss: 5.6508 - output_1_loss: 2.8772 - output_2_loss: 2.7737\n","Epoch 2/3\n","5427/5427 [==============================] - 5621s 1s/step - loss: 2.7055 - output_1_loss: 1.3828 - output_2_loss: 1.3227\n","Epoch 3/3\n","5427/5427 [==============================] - 5633s 1s/step - loss: 2.2689 - output_1_loss: 1.1664 - output_2_loss: 1.1025\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fc1ca06cf28>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xUA6UP1LVN4x","executionInfo":{"status":"ok","timestamp":1609773729240,"user_tz":-330,"elapsed":1016,"user":{"displayName":"TINA GHOSH","photoUrl":"","userId":"17633624193521379699"}},"outputId":"0c2ae91f-0413-41b4-f415-e95a92bbf7d4"},"source":["question, text = \"Who worked with the poor in the India?\", \"Mother Teresa, the Roman Catholic nun who worked with the poor in the Indian city of Kolkata (Calcutta),is being declared a saint. The order she founded, the Missionaries of Charity, has grown to include 4,500 nuns and 400 brothers in 87 countries, tending to the poor and dying in the slums of 160 cities.\"\n","input_dict = tokenizer(question, text, return_tensors=\"tf\")\n","outputs = model(input_dict, return_dict=True)\n","start_logits = outputs.start_logits\n","end_logits = outputs.end_logits\n","all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n","answer = ' '.join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0]+1])\n","print(answer)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["mother teresa\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GNC3hPI7SJ7Y","executionInfo":{"status":"ok","timestamp":1609773878408,"user_tz":-330,"elapsed":753,"user":{"displayName":"TINA GHOSH","photoUrl":"","userId":"17633624193521379699"}},"outputId":"b16a4d89-a30f-40cf-9481-76f685847d0b"},"source":["question, text = \"How many times Indian cricket team was World Champions?\", \"The Indian cricket team are two times World Champions.In addition to winning the 1983 Cricket World Cup, they triumphed over Sri Lanka in the 2011 Cricket World Cup on home soil.\"\n","input_dict = tokenizer(question, text, return_tensors=\"tf\")\n","outputs = model(input_dict, return_dict=True)\n","start_logits = outputs.start_logits\n","end_logits = outputs.end_logits\n","all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n","answer = ' '.join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0]+1])\n","print(answer)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["two\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"om7iP3kRRxaX","executionInfo":{"status":"ok","timestamp":1609773902394,"user_tz":-330,"elapsed":742,"user":{"displayName":"TINA GHOSH","photoUrl":"","userId":"17633624193521379699"}},"outputId":"11633a50-2e51-4704-9401-3b392932cf4d"},"source":["question, text = \"Which state has first confirm case of COVID-19?\", \"The first case of the COVID-19 pandemic in the Indian state of Karnataka was confirmed on 9 March 2020. Two days later, the state became the first in India to invoke the provisions of the Epidemic Diseases Act, 1897, which are set to last for a year, to curb the spread of the disease.Till date, there have been 841,889 (6 November 2020) confirmed cases with 797,204 (6 November 2020) recoveries and 11,347 (6 November 2020) deaths in the state.[3]\"\n","input_dict = tokenizer(question, text, return_tensors=\"tf\")\n","outputs = model(input_dict, return_dict=True)\n","start_logits = outputs.start_logits\n","end_logits = outputs.end_logits\n","all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n","answer = ' '.join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0]+1])\n","print(answer)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["karnataka\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YtBsvVsLSLfq","executionInfo":{"status":"ok","timestamp":1609773921212,"user_tz":-330,"elapsed":1204,"user":{"displayName":"TINA GHOSH","photoUrl":"","userId":"17633624193521379699"}},"outputId":"46eb564e-a5be-4142-f267-f8dc469d4866"},"source":["question, text = \"Which year spanish flu came?\", \"The Spanish flu, also known as the 1918 flu pandemic, was an unusually deadly influenza pandemic caused by the H1N1 influenza  A virus. Lasting from February 1918 to April 1920, it infected 500 million people – about a third of the world's population at the time – in four successive waves.\"\n","input_dict = tokenizer(question, text, return_tensors=\"tf\")\n","outputs = model(input_dict, return_dict=True)\n","start_logits = outputs.start_logits\n","end_logits = outputs.end_logits\n","all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n","answer = ' '.join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0]+1])\n","print(answer)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["1918 flu pan ##de ##mic , was an unusually deadly influenza pan ##de ##mic caused by the h ##1 ##n ##1 influenza a virus . lasting from february 1918 to april 1920\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GWE_OMEnSMNf","executionInfo":{"status":"ok","timestamp":1609773972342,"user_tz":-330,"elapsed":784,"user":{"displayName":"TINA GHOSH","photoUrl":"","userId":"17633624193521379699"}},"outputId":"8435f5e0-fd66-46b2-9f89-cfd415245cc5"},"source":["question, text = \"How many seasons there in India?\", \"Seasons remind us that change is the law of nature and a sign of progress. In India, there are mainly six seasons as per the ancient Hindu calendar (the Lunisolar Hindu). The twelve months in a year are divided into six seasons of two-month duration each. These seasons include Vasant Ritu (Spring), Grishma Ritu (Summer), Varsha Ritu (Monsoon), Sharad Ritu (Autumn), Hemant Ritu (Pre-Winter) and Shishir Ritu (Winter). However, as per the India Meteorological Department (IMD), there are four seasons in India like other parts of the world.\"\n","input_dict = tokenizer(question, text, return_tensors=\"tf\")\n","outputs = model(input_dict, return_dict=True)\n","start_logits = outputs.start_logits\n","end_logits = outputs.end_logits\n","all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n","answer = ' '.join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0]+1])\n","print(answer)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["six\n"],"name":"stdout"}]}]}